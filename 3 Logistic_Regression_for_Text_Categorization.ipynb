{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Regression_for_Text_Categorization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsYAMZwocQp4",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression for Text Categorization\n",
        "\n",
        "In this document, we will do experiments using Logistic Regression algorithm for text classification task. We will use the framework sklearn for experiments.\n",
        "\n",
        "For the binary classification, we will re-use the sentiment classification data. For multi-class classification, we will use the 20 newsgroups dataset. It will be automatically downloaded, then cached.\n",
        "\n",
        "## Binary classification\n",
        "\n",
        "We download the data set as the first step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B69LTgEec0Si",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "399b51da-066b-4db9-b584-f8f06adf4ded"
      },
      "source": [
        "!rm -f sentiment.txt\n",
        "!wget https://raw.githubusercontent.com/minhpqn/nlp_100_drill_exercises/master/data/sentiment.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-15 04:15:42--  https://raw.githubusercontent.com/minhpqn/nlp_100_drill_exercises/master/data/sentiment.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1270444 (1.2M) [text/plain]\n",
            "Saving to: ‘sentiment.txt’\n",
            "\n",
            "sentiment.txt       100%[===================>]   1.21M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-03-15 04:15:45 (16.2 MB/s) - ‘sentiment.txt’ saved [1270444/1270444]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE_vvHs6dfCW",
        "colab_type": "text"
      },
      "source": [
        "### Load data\n",
        "\n",
        "We will load data into a list of sentences with their labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EaRT5e1dn0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line == \"\":\n",
        "                continue\n",
        "            match = re.search(r\"(\\+1|-1)[\\s\\t]+(.+)$\", line)  # match the line +1 ...\n",
        "            if match:\n",
        "                lb = match.group(1)\n",
        "                sentence = match.group(2)\n",
        "                if sentence == \"\":\n",
        "                    continue\n",
        "                data.append((sentence,lb))\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KslXet-rdrjU",
        "colab_type": "text"
      },
      "source": [
        "We will use the above function to load sentiment data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b3uhb7udv6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be5148c4-3155-4ea5-aca6-dfdd17beeed8"
      },
      "source": [
        "DATA_PATH = \"./sentiment.txt\"\n",
        "data = load_data(DATA_PATH)\n",
        "\n",
        "print(\"# Loaded {} examples\".format(len(data)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Loaded 10662 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0rulybydy0q",
        "colab_type": "text"
      },
      "source": [
        "We also split data into training/test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIM167ZYd3TE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c04410fd-13d4-40a1-8ce1-cb858ec2f0a9"
      },
      "source": [
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = load_data(DATA_PATH)\n",
        "docs, labels = zip(*data)\n",
        "\n",
        "train_docs, test_docs, train_labels, test_labels = train_test_split(docs, labels,\n",
        "                                                                   test_size=0.2,\n",
        "                                                                   random_state=42)\n",
        "print(\"Training reviews: {}\".format(len(train_docs)))\n",
        "print(\"Test reviews: {}\".format(len(test_docs)))\n",
        "\n",
        "# Let's see some positive and negative documents in test data.\n",
        "posi_docs = []\n",
        "neg_docs = []\n",
        "for d, lb in zip(test_docs, test_labels):\n",
        "    if lb == \"+1\":\n",
        "        posi_docs.append(d)\n",
        "    else:\n",
        "        neg_docs.append(d)\n",
        "\n",
        "print(\"Random positive review\")\n",
        "print(random.choice(posi_docs))\n",
        "print(\"Random negative review\")\n",
        "print(random.choice(neg_docs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training reviews: 8529\n",
            "Test reviews: 2133\n",
            "Random positive review\n",
            "the story ultimately takes hold and grips hard .\n",
            "Random negative review\n",
            "first-time writer-director dylan kidd also has a good ear for dialogue , and the characters sound like real people .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmA_faq5d6oF",
        "colab_type": "text"
      },
      "source": [
        "### Using scikit-learn for feature extraction\n",
        "\n",
        "We can use scikit-learn for [feature extraction](http://scikit-learn.org/stable/modules/feature_extraction.html). We use the bag-of-word representation for feature extraction. In scikit-learn, we can use `CountVectorizer` or `TfidfTransformer`.\n",
        "\n",
        "### Feature extraction with CountVectorizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvfGX24PeLms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b45fef96-9587-48a9-de85-f3b10f0ab7db"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "                             binary=True,  # Use binary features\n",
        "                            ) \n",
        "vectorizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2ymNE8PePXa",
        "colab_type": "text"
      },
      "source": [
        "Now, we fit the vectorizer object on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vX0-2s7eSiC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b35821ce-2129-4850-dc81-e49b0763f690"
      },
      "source": [
        "X_train = vectorizer.fit_transform(train_docs)\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8529, 16530)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7ouScuXeUEE",
        "colab_type": "text"
      },
      "source": [
        "We we try the vectorizer to get BoW of a sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UxomvFDeWHk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "14bad455-dd9c-4934-a380-f5ef96f903ca"
      },
      "source": [
        "analyze = vectorizer.build_analyzer()\n",
        "analyze(\"This is a text document to analyze.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', 'text', 'document', 'to', 'analyze']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bji1eO_AeXg5",
        "colab_type": "text"
      },
      "source": [
        "### Text categorization with logistic regression\n",
        "\n",
        "Now let's try text categorization with [logistic regression implementation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) in scikit-learn. See the document [here](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOe_mZSvecIM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "bc83010a-c2d3-48ef-d01b-7a624fb10e5c"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=500)\n",
        "clf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83t_d-QXey2R",
        "colab_type": "text"
      },
      "source": [
        "Now, we fit the model on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxT21G5Ue1Ug",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "33aed803-fdd4-4332-cabf-970167506ddf"
      },
      "source": [
        "clf.fit(X_train, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9HVDkGMe2xD",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation on test set\n",
        "\n",
        "Now let's evaluate the model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmnGcUuwe6JB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = vectorizer.transform(test_docs)\n",
        "test_preds = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gId4ZRsWe99l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79914d3f-3045-4c87-e1ca-b3cf8cfa4d64"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "accuracy = metrics.accuracy_score(test_labels, test_preds)\n",
        "print(\"# Test accuracy: {}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Test accuracy: 0.753398968588842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAKlKMXjfdrM",
        "colab_type": "text"
      },
      "source": [
        "See the classification report:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ySjIWuuff8G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "25b9ac0b-3fab-4919-e5bb-133592f21ce8"
      },
      "source": [
        "print( metrics.classification_report(test_labels, test_preds) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          +1       0.76      0.74      0.75      1062\n",
            "          -1       0.75      0.77      0.76      1071\n",
            "\n",
            "    accuracy                           0.75      2133\n",
            "   macro avg       0.75      0.75      0.75      2133\n",
            "weighted avg       0.75      0.75      0.75      2133\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k0P77WufEej",
        "colab_type": "text"
      },
      "source": [
        "We can predict the label for an input review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrZf2O4cfG2m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "56a26c59-f27e-4d88-c125-cd132b4f92b4"
      },
      "source": [
        "example = \"a thoughtful , provocative , insistently humanizing film .\"\n",
        "test_x = vectorizer.transform([example])\n",
        "print(\"Predicted class: {}\".format(clf.predict(test_x)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class: ['+1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbtQpM1xfIr_",
        "colab_type": "text"
      },
      "source": [
        "We can get prediction probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNLGkjiAfNk0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "baae8c82-bbc3-4ffa-df5f-3c685ec59659"
      },
      "source": [
        "clf.predict_proba(test_x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.85279457, 0.14720543]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNxASF-zfSUR",
        "colab_type": "text"
      },
      "source": [
        "The first value is the probability that the instance belongs to the class \"+1\" and the second value is the probability that the instance belongs to the class \"-1\". Let's try a negative review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH288aYPfU-S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1a1d07d7-6ab4-4feb-b2e3-1d9592255cc8"
      },
      "source": [
        "example2 = \"for all its surface frenzy , high crimes should be charged with loitering -- so much on view , so little to offer .\"\n",
        "test_x2 = vectorizer.transform([example2])\n",
        "clf.predict_proba(test_x2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.14283192, 0.85716808]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq-DqtDpfWom",
        "colab_type": "text"
      },
      "source": [
        "We can combine probability values with a threshold $t$ to customize our prediction. For instance, we can decide that the prediction is \"+1\" if the probability is greater than 0.6 instead of 0.5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sokkOpbnftrg",
        "colab_type": "text"
      },
      "source": [
        "### Get top features with the highest weights\n",
        "\n",
        "In this section, we would like to see top features with the highest weights.\n",
        "\n",
        "First, we get all features in vectorizer and target_names.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGTadetvgAZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "36bf11a2-5c2a-46b2-ecc2-6b37252c1602"
      },
      "source": [
        "feature_names = vectorizer.get_feature_names()\n",
        "target_names = [\"+1\", \"-1\"]\n",
        "print(len(clf.coef_), clf.coef_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 [[ 0.08720718  0.05923897  0.05923897 ...  0.03480673 -0.00243007\n",
            "   0.0372368 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-KOQ_9zfwfB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7f694215-0db2-40c2-d554-e46c9a9c0510"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "topN = 50\n",
        "print(\"top {} keywords:\".format(topN))\n",
        "top10 = np.argsort(clf.coef_[0])[-topN:]\n",
        "top_features = [ feature_names[i] for i in top10 ]\n",
        "print(\" \".join(top_features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top 50 keywords:\n",
            "obvious junk stupid choppy tired has its none already plain premise video wrong awful unfortunately has all shallow plays like script jokes badly the worst pointless doesn ill thin suffers bland plodding unpleasant fails routine tv tedious generic pretentious mediocre bore mildly heavy flat neither only unfunny mess lacks worst bad boring too dull\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqqzlMfsfzDL",
        "colab_type": "text"
      },
      "source": [
        "### Try with tf-idf term weighting\n",
        "\n",
        "Now, we use tf-idf term weighting for feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLbICHDbgSSk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1877cace-46c5-4567-af2d-0ae7e5b180d3"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
        "                             stop_words='english')\n",
        "X_train = vectorizer.fit_transform(train_docs)\n",
        "\n",
        "clf = LogisticRegression(solver='lbfgs')\n",
        "\n",
        "clf.fit(X_train, train_labels)\n",
        "\n",
        "X_test = vectorizer.transform(test_docs)\n",
        "test_preds = clf.predict(X_test)\n",
        "\n",
        "accuracy = metrics.accuracy_score(test_labels, test_preds)\n",
        "print(\"# Test accuracy: {}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Test accuracy: 0.7510548523206751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxXVMxqFgUfE",
        "colab_type": "text"
      },
      "source": [
        "## Multiclass Text Classification\n",
        "\n",
        "In this section, we will do multiclass text classification with 20 newsgroup dataset. It will be automatically downloaded, then cached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCxhx1LNg0Xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "remove = ('headers', 'footers', 'quotes')\n",
        "\n",
        "data_train = fetch_20newsgroups(subset='train',\n",
        "                                shuffle=True, random_state=42,\n",
        "                                remove=remove)\n",
        "\n",
        "data_test = fetch_20newsgroups(subset='test',\n",
        "                               shuffle=True, random_state=42,\n",
        "                               remove=remove)\n",
        "\n",
        "y_train, y_test = data_train.target, data_test.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOPgKopEhQEs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "745076f5-ae1c-46a4-972f-868e36d373b9"
      },
      "source": [
        "def size_mb(docs):\n",
        "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
        "\n",
        "data_train_size_mb = size_mb(data_train.data)\n",
        "data_test_size_mb = size_mb(data_test.data)\n",
        "\n",
        "print(\"%d documents - %0.3fMB (training set)\" % (\n",
        "    len(data_train.data), data_train_size_mb))\n",
        "print(\"%d documents - %0.3fMB (test set)\" % (\n",
        "    len(data_test.data), data_test_size_mb))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11314 documents - 13.782MB (training set)\n",
            "7532 documents - 8.262MB (test set)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgwSQlm7qwib",
        "colab_type": "text"
      },
      "source": [
        "### Feature Extraction\n",
        "\n",
        "We will use TF-IDF features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgWEx7JWq1un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
        "                             stop_words='english')\n",
        "X_train = vectorizer.fit_transform(data_train.data)\n",
        "X_test = vectorizer.transform(data_test.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRjrAQ71rWJx",
        "colab_type": "text"
      },
      "source": [
        "Let's try Logistic Regression with 'ovr' (one-vs-rest) strategy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv6tpNj1rje0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = LogisticRegression(solver='lbfgs', multi_class='ovr')\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDzbmy-8r0ji",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate the results on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UI5rhndr4T2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f0924d2-f116-404b-9c8e-e97f5d987875"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "y_preds = clf.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test, y_preds)\n",
        "print(\"# Test accuracy: {}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Test accuracy: 0.6949017525225704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZCAcFN8sET2",
        "colab_type": "text"
      },
      "source": [
        "Let's try multinomial Logistic Regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yqXTtsXsMIp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "172f0544-047c-4782-b824-a4f4aab7c965"
      },
      "source": [
        "clf = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vko1xyrOsRUF",
        "colab_type": "text"
      },
      "source": [
        "We will test multinomial Logistic Regression on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubhdUJ6WsdAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44cc7c1b-16d1-490f-df2b-7556248d4a13"
      },
      "source": [
        "y_preds = clf.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test, y_preds)\n",
        "print(\"# Test accuracy: {}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Test accuracy: 0.6946362187997875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EfI4dy6sf-Z",
        "colab_type": "text"
      },
      "source": [
        "### SGDClassifier with log loss\n",
        "\n",
        "We will use [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) with logistic loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGnPwLaOsvVK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "d2a1f502-14d3-4657-8a1c-8433fdd80b97"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "clf2 = SGDClassifier(alpha=.0001, max_iter=100, loss='log',\n",
        "                     penalty='l2')\n",
        "clf2.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
              "              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=100,\n",
              "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
              "              random_state=None, shuffle=True, tol=0.001,\n",
              "              validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fEDzoc2tUcY",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate SGDClassifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC3X5mJatgNx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e8d7283-f986-4649-f026-665ea44fb57e"
      },
      "source": [
        "y_preds = clf2.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test, y_preds)\n",
        "print(\"# Test accuracy: {}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Test accuracy: 0.6935740839086564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJWFjkvDthJn",
        "colab_type": "text"
      },
      "source": [
        "## Naive Bayes Classifier\n",
        "\n",
        "We will compare the result of Logistic Regression with Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vztZigZdt-SR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "846b1439-2c3c-4d46-b216-d891ba2640bc"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf3 = MultinomialNB(alpha=.01)\n",
        "clf3.fit(X_train, y_train)\n",
        "y_preds = clf3.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test, y_preds)\n",
        "print(\"# Test accuracy: {}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Test accuracy: 0.6964949548592672\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}