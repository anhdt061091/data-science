{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaning_and_processing_text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwh4NjStYSXs",
        "colab_type": "text"
      },
      "source": [
        "# Cleaning and processing text data\n",
        "\n",
        "## Objectives\n",
        "\n",
        "In this documents, we will show:\n",
        "\n",
        "- How to clean text data with regular expression\n",
        "- How to processing text data with NLP toolkits (nltk and spacy).\n",
        "\n",
        "\n",
        "## Cleaning textual data\n",
        "\n",
        "### Demo data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQdEmbc_aH6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent1 = \"Two explosions were just reported at a flood-damaged chemical plant near\\u00a0Houston https://t.co/jTIZXoKr2B https://t.co/u0gTIsaHIy\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmcrVw7ecfdm",
        "colab_type": "text"
      },
      "source": [
        "### Remove URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftlPznVrcjNk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "24c36f5e-2f56-46d5-af1d-fd8aba47c809"
      },
      "source": [
        "import re\n",
        "\n",
        "# When the UNICODE flag is not specified, matches any non-whitespace character\n",
        "result = re.sub(r\"http\\S+\", \"\", sent1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Two explosions were just reported at a flood-damaged chemical plant nearÂ Houston  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HutRCizSboPe",
        "colab_type": "text"
      },
      "source": [
        "### Remove punctuations in text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP2z0aArbqsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEO2g70KhpOy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "d7c9c022-5a62-47b3-e7de-f9cba39ea540"
      },
      "source": [
        "help(str.maketrans)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on built-in function maketrans:\n",
            "\n",
            "maketrans(x, y=None, z=None, /)\n",
            "    Return a translation table usable for str.translate().\n",
            "    \n",
            "    If there is only one argument, it must be a dictionary mapping Unicode\n",
            "    ordinals (integers) or characters to Unicode ordinals, strings or None.\n",
            "    Character keys will be then converted to ordinals.\n",
            "    If there are two arguments, they must be strings of equal length, and\n",
            "    in the resulting dictionary, each character in x will be mapped to the\n",
            "    character at the same position in y. If there is a third argument, it\n",
            "    must be a string, whose characters will be mapped to None in the result.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3C0B0M_bu_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b134896-5875-4a63-9d99-fa5d6f149f89"
      },
      "source": [
        "sent2 = \"Stories of Pittsburghers trapped in #Houston flooding!!!! @@ - https://t.co/j5igfpvLJu https://t.co/8gsUpD8jsa\"\n",
        "sent2_ = re.sub(r\"http\\S+\", \"\", sent2)\n",
        "print(sent2_)\n",
        "\n",
        "translator = str.maketrans('', '', string.punctuation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stories of Pittsburghers trapped in #Houston flooding!!!! @@ -  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6h7opF4cI0T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6cbff968-ecf5-4175-abc0-53cbfdf24de0"
      },
      "source": [
        "sent2_pun = sent2_.translate(translator)\n",
        "print(sent2_pun)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stories of Pittsburghers trapped in Houston flooding    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KRSQboecUIp",
        "colab_type": "text"
      },
      "source": [
        "## Processing text data with NLP\n",
        "\n",
        "\n",
        "### Sentence tokenization\n",
        "\n",
        "In text mining task, we may want to split long articles into sentences. We can do that with nltk and spacy.\n",
        "\n",
        "Demo data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMsYcJBieZdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "para = \"Hello World. It's good to see you. Thanks for buying this book.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIIHpsWaeau7",
        "colab_type": "text"
      },
      "source": [
        "#### Using NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sRVyxzpgJKq",
        "colab_type": "text"
      },
      "source": [
        "If your have not yet downloaded the tokenization model, you should do that before processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duuf4nnwgWNG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "ed819ee2-d442-42e8-d453-307b9af2458d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYW3aWfOgW5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8b3fe792-fc5a-41c1-b494-90334a782436"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(para)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZbrAQBbgajw",
        "colab_type": "text"
      },
      "source": [
        "#### Using spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8NfCV3JhCrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIQdxeZHggvt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5e142ffb-61a8-4edc-d2d0-337d86e99281"
      },
      "source": [
        "doc = nlp(para)\n",
        "sents = [sent.text for sent in doc.sents]\n",
        "print(sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paBwa3EYg-tI",
        "colab_type": "text"
      },
      "source": [
        "### Word tokenization\n",
        "\n",
        "Demo data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5MGG7TGhkZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent = 'The history of NLP generally starts in the 1950s, although work can be found from earlier periods.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17GhDIa6hnUa",
        "colab_type": "text"
      },
      "source": [
        "#### Using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTNJOT9vhqBz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "43d9707e-61a1-498c-b664-a8ab144a8556"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print( word_tokenize(sent) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'history', 'of', 'NLP', 'generally', 'starts', 'in', 'the', '1950s', ',', 'although', 'work', 'can', 'be', 'found', 'from', 'earlier', 'periods', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JOdMSE8hsd0",
        "colab_type": "text"
      },
      "source": [
        "#### Using spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTCoT5VRhvjm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8a109142-0dec-4e83-e829-3f6397058f3d"
      },
      "source": [
        "doc = nlp(sent)\n",
        "tokens = [x.text for x in doc]\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'history', 'of', 'NLP', 'generally', 'starts', 'in', 'the', '1950s', ',', 'although', 'work', 'can', 'be', 'found', 'from', 'earlier', 'periods', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtZPvlx0iAah",
        "colab_type": "text"
      },
      "source": [
        "### POS Tagging\n",
        "\n",
        "\n",
        "Demo data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AowwZuliK9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent = 'The history of NLP generally starts in the 1950s, although work can be found from earlier periods.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf4hM7yGiiqj",
        "colab_type": "text"
      },
      "source": [
        "#### Using NLTK\n",
        "\n",
        "First we need to download POS tagger model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9MWkt4zin6B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "2ab84631-94a7-4701-e7fa-49f3a7aaa512"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjkXbmbSirKU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "e1c7c1fe-cada-4d51-91bf-7e74135dd334"
      },
      "source": [
        "tokens = word_tokenize(sent)\n",
        "nltk.pos_tag(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('history', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('NLP', 'NNP'),\n",
              " ('generally', 'RB'),\n",
              " ('starts', 'VBZ'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('1950s', 'CD'),\n",
              " (',', ','),\n",
              " ('although', 'IN'),\n",
              " ('work', 'NN'),\n",
              " ('can', 'MD'),\n",
              " ('be', 'VB'),\n",
              " ('found', 'VBN'),\n",
              " ('from', 'IN'),\n",
              " ('earlier', 'JJR'),\n",
              " ('periods', 'NNS'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh-pOp7Di2Rr",
        "colab_type": "text"
      },
      "source": [
        "#### Using spacy\n",
        "\n",
        "spacy use universal part-of-speech tags, but we can map that tagset to English Penntree bank tagset. See [https://spacy.io/api/annotation](https://spacy.io/api/annotation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2DoWHDGi8T4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "4cc6cb82-c21f-4739-ee4b-fa139fe69bb6"
      },
      "source": [
        "doc = nlp(sent)\n",
        "[(x.text, x.pos_) for x in doc]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DET'),\n",
              " ('history', 'NOUN'),\n",
              " ('of', 'ADP'),\n",
              " ('NLP', 'PROPN'),\n",
              " ('generally', 'ADV'),\n",
              " ('starts', 'VERB'),\n",
              " ('in', 'ADP'),\n",
              " ('the', 'DET'),\n",
              " ('1950s', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('although', 'SCONJ'),\n",
              " ('work', 'NOUN'),\n",
              " ('can', 'VERB'),\n",
              " ('be', 'AUX'),\n",
              " ('found', 'VERB'),\n",
              " ('from', 'ADP'),\n",
              " ('earlier', 'ADJ'),\n",
              " ('periods', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVs9owr7jJLb",
        "colab_type": "text"
      },
      "source": [
        "### Word Lemmatization\n",
        "\n",
        "\n",
        "#### Using NLTK\n",
        "\n",
        "We need to download wordnet model as first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxwEdlB0kBaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5f5ff626-d877-438c-e34b-7a7a8a0dd30e"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkE1pD1YkgHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLTNzqvsk9me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDXzgaSplPsu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "a53d3e04-be3b-46d1-c10b-f27581f53af5"
      },
      "source": [
        "sent = 'The history of NLP generally starts in the 1950s, although work can be found from earlier periods.'\n",
        "\n",
        "tokens = nltk.word_tokenize(sent)\n",
        "tags = nltk.pos_tag(tokens)\n",
        "\n",
        "for i, token in enumerate(tokens):\n",
        "    pos_tag = tags[i][1]\n",
        "    if pos_tag.startswith('N'):\n",
        "        lemma = lemmatizer.lemmatize(token, pos=NOUN)\n",
        "    elif pos_tag.startswith('V'):\n",
        "        lemma = lemmatizer.lemmatize(token, pos=VERB)\n",
        "    elif pos_tag.startswith('J'):\n",
        "        lemma = lemmatizer.lemmatize(token, pos=ADJ)\n",
        "    else:\n",
        "        lemma = token\n",
        "    print(\"%s - %s\" % (token, lemma))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The - The\n",
            "history - history\n",
            "of - of\n",
            "NLP - NLP\n",
            "generally - generally\n",
            "starts - start\n",
            "in - in\n",
            "the - the\n",
            "1950s - 1950s\n",
            ", - ,\n",
            "although - although\n",
            "work - work\n",
            "can - can\n",
            "be - be\n",
            "found - find\n",
            "from - from\n",
            "earlier - early\n",
            "periods - period\n",
            ". - .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aXdDxHgmLBe",
        "colab_type": "text"
      },
      "source": [
        "#### Using spacy\n",
        "\n",
        "Lemmatization is much easier in spacy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AiYtywZmt-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "bbc59742-92ad-45eb-b3bb-e6f0ae356d46"
      },
      "source": [
        "doc = nlp(sent)\n",
        "[(x.text, x.lemma_) for x in doc]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'the'),\n",
              " ('history', 'history'),\n",
              " ('of', 'of'),\n",
              " ('NLP', 'NLP'),\n",
              " ('generally', 'generally'),\n",
              " ('starts', 'start'),\n",
              " ('in', 'in'),\n",
              " ('the', 'the'),\n",
              " ('1950s', '1950'),\n",
              " (',', ','),\n",
              " ('although', 'although'),\n",
              " ('work', 'work'),\n",
              " ('can', 'can'),\n",
              " ('be', 'be'),\n",
              " ('found', 'find'),\n",
              " ('from', 'from'),\n",
              " ('earlier', 'early'),\n",
              " ('periods', 'period'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7QoLgQgm0T0",
        "colab_type": "text"
      },
      "source": [
        "### Filtering stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2Fz9wgXnJOF",
        "colab_type": "text"
      },
      "source": [
        "#### Using NLTK\n",
        "\n",
        "We need to download list of stop words first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycgpYSNynVMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "eb510df7-1723-47f6-b78e-f81de05d9975"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tge97cNGnVkl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "e43d364c-eace-405d-a2e2-e804effc3a20"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "english_stops = set(stopwords.words('english'))\n",
        "\n",
        "sent = 'The history of NLP generally starts in the 1950s, although work can be found from earlier periods.'\n",
        "\n",
        "words = nltk.word_tokenize(sent)\n",
        "[word for word in words if word not in english_stops]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'history',\n",
              " 'NLP',\n",
              " 'generally',\n",
              " 'starts',\n",
              " '1950s',\n",
              " ',',\n",
              " 'although',\n",
              " 'work',\n",
              " 'found',\n",
              " 'earlier',\n",
              " 'periods',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw9Ekc-9nvky",
        "colab_type": "text"
      },
      "source": [
        "#### Using spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLTbHf45oN0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "6209c495-14ff-4edd-e7b5-206d87f173ca"
      },
      "source": [
        "spacy_stopwords = set(spacy.lang.en.stop_words.STOP_WORDS)\n",
        "sent = 'The history of NLP generally starts in the 1950s, although work can be found from earlier periods.'\n",
        "doc = nlp(sent)\n",
        "words = [x.text for x in doc]\n",
        "[word for word in words if word not in spacy_stopwords]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'history',\n",
              " 'NLP',\n",
              " 'generally',\n",
              " 'starts',\n",
              " '1950s',\n",
              " ',',\n",
              " 'work',\n",
              " 'found',\n",
              " 'earlier',\n",
              " 'periods',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Oih0-KBokO3",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "1. [spaCy 101: Everything you need to know](https://spacy.io/usage/spacy-101)\n",
        "2. [Advanced NLP with spacy](https://course.spacy.io/)\n",
        "3. Bird, Steven; Klein, Ewan; Loper, Edward (2009). *Natural Language Processing with Python*. [http://www.nltk.org/book/](http://www.nltk.org/book/)\n",
        "4. [NLTK in 20 minutes](http://www.slideshare.net/japerk/nltk-in-20-minutes), by Jacob Perkins\n"
      ]
    }
  ]
}